#!/usr/bin/env python3
"""
å…ƒæ•°æ®æ¸…ç†å·¥å…·è„šæœ¬
ç”¨äºæ‰‹åŠ¨æ‰§è¡Œå…ƒæ•°æ®ä¸€è‡´æ€§æ£€æŸ¥å’Œæ¸…ç†æ“ä½œ
"""

import asyncio
import argparse
import json
from pathlib import Path
from datetime import datetime
from metadata_config import get_metadata_manager
from metadata_cleanup_manager import MetadataCleanupManager


async def check_consistency(storage_root: str):
    """æ£€æŸ¥å…ƒæ•°æ®ä¸€è‡´æ€§"""
    print("ğŸ” å¼€å§‹æ£€æŸ¥å…ƒæ•°æ®ä¸€è‡´æ€§...")
    
    manager = get_metadata_manager(storage_root)
    cleanup_manager = MetadataCleanupManager(manager)
    await cleanup_manager.initialize()
    
    result = await cleanup_manager.check_consistency()
    
    print(f"\nğŸ“Š ä¸€è‡´æ€§æ£€æŸ¥ç»“æœ:")
    print(f"   æ£€æŸ¥æ–‡ä»¶æ•°: {result['files_checked']}")
    print(f"   å­¤å„¿å…ƒæ•°æ®: {len(result['orphan_metadata'])}")
    print(f"   ç¼ºå¤±å…ƒæ•°æ®: {len(result['missing_metadata'])}")
    print(f"   é”™è¯¯æ•°é‡: {len(result['errors'])}")
    
    if result['orphan_metadata']:
        print(f"\nğŸš¨ å‘ç°å­¤å„¿å…ƒæ•°æ®:")
        for i, orphan in enumerate(result['orphan_metadata'][:10], 1):
            print(f"   {i:3d}. {orphan['file_path']}")
        if len(result['orphan_metadata']) > 10:
            print(f"   ... è¿˜æœ‰ {len(result['orphan_metadata']) - 10} ä¸ª")
    
    if result['missing_metadata']:
        print(f"\nâš ï¸  å‘ç°ç¼ºå¤±å…ƒæ•°æ®çš„æ–‡ä»¶:")
        for i, missing in enumerate(result['missing_metadata'][:10], 1):
            print(f"   {i:3d}. {missing}")
        if len(result['missing_metadata']) > 10:
            print(f"   ... è¿˜æœ‰ {len(result['missing_metadata']) - 10} ä¸ª")
    
    if result['errors']:
        print(f"\nâŒ æ£€æŸ¥è¿‡ç¨‹ä¸­çš„é”™è¯¯:")
        for error in result['errors'][:5]:
            print(f"   - {error}")
        if len(result['errors']) > 5:
            print(f"   ... è¿˜æœ‰ {len(result['errors']) - 5} ä¸ªé”™è¯¯")
    
    return result


async def cleanup_orphans(storage_root: str, dry_run: bool = True, max_orphans: int = None):
    """æ¸…ç†å­¤å„¿å…ƒæ•°æ®"""
    action = "è¯•è¿è¡Œæ¸…ç†" if dry_run else "æ‰§è¡Œæ¸…ç†"
    print(f"ğŸ§¹ å¼€å§‹{action}å­¤å„¿å…ƒæ•°æ®...")
    
    manager = get_metadata_manager(storage_root)
    cleanup_manager = MetadataCleanupManager(manager)
    await cleanup_manager.initialize()
    
    result = await cleanup_manager.cleanup_orphan_metadata(dry_run=dry_run, max_orphans=max_orphans)
    
    print(f"\nğŸ“Š æ¸…ç†ç»“æœ:")
    print(f"   æ¸…ç†ç±»å‹: {result.cleanup_type}")
    print(f"   æ£€æŸ¥æ–‡ä»¶æ•°: {result.files_checked}")
    print(f"   å‘ç°å­¤å„¿: {result.orphans_found}")
    print(f"   æ¸…ç†å­¤å„¿: {result.orphans_cleaned}")
    print(f"   é”™è¯¯æ•°é‡: {result.errors}")
    print(f"   æ‰§è¡Œæ—¶é—´: {result.duration:.2f}ç§’")
    
    if dry_run and result.orphans_found > 0:
        print(f"\nâš ï¸  è¿™æ˜¯è¯•è¿è¡Œæ¨¡å¼ï¼Œæ²¡æœ‰å®é™…åˆ é™¤å…ƒæ•°æ®")
        print(f"   è¦æ‰§è¡Œå®é™…æ¸…ç†ï¼Œè¯·æ·»åŠ  --execute å‚æ•°")
        
        if "orphan_files" in result.details:
            print(f"\nğŸ“‹ å°†è¦æ¸…ç†çš„å­¤å„¿å…ƒæ•°æ®:")
            for i, file_path in enumerate(result.details["orphan_files"][:10], 1):
                print(f"   {i:3d}. {file_path}")
            if len(result.details["orphan_files"]) > 10:
                print(f"   ... è¿˜æœ‰ {len(result.details['orphan_files']) - 10} ä¸ª")
    
    elif not dry_run and result.orphans_cleaned > 0:
        print(f"\nâœ… æˆåŠŸæ¸…ç†äº† {result.orphans_cleaned} ä¸ªå­¤å„¿å…ƒæ•°æ®")
        
        if "backup" in result.details:
            backup_info = result.details["backup"]
            print(f"   å¤‡ä»½æ–‡ä»¶: {backup_info['backup_file']}")
    
    if result.errors > 0:
        print(f"\nâŒ æ¸…ç†è¿‡ç¨‹ä¸­å‡ºç° {result.errors} ä¸ªé”™è¯¯")
        if "errors" in result.details:
            for error in result.details["errors"][:3]:
                print(f"   - {error}")
    
    return result


async def show_stats(storage_root: str, days: int = 7):
    """æ˜¾ç¤ºæ¸…ç†ç»Ÿè®¡ä¿¡æ¯"""
    print(f"ğŸ“ˆ æ˜¾ç¤ºæœ€è¿‘ {days} å¤©çš„æ¸…ç†ç»Ÿè®¡...")
    
    manager = get_metadata_manager(storage_root)
    cleanup_manager = MetadataCleanupManager(manager)
    await cleanup_manager.initialize()
    
    stats = await cleanup_manager.get_cleanup_stats(days)
    
    print(f"\nğŸ“Š æ¸…ç†ç»Ÿè®¡ (æœ€è¿‘ {days} å¤©):")
    print(f"   æ€»è¿è¡Œæ¬¡æ•°: {stats['total_runs']}")
    print(f"   æ£€æŸ¥æ–‡ä»¶æ€»æ•°: {stats['total_files_checked']}")
    print(f"   å‘ç°å­¤å„¿æ€»æ•°: {stats['total_orphans_found']}")
    print(f"   æ¸…ç†å­¤å„¿æ€»æ•°: {stats['total_orphans_cleaned']}")
    print(f"   é”™è¯¯æ€»æ•°: {stats['total_errors']}")
    print(f"   å¹³å‡æ‰§è¡Œæ—¶é—´: {stats['avg_duration']:.2f}ç§’")
    
    if stats['recent_logs']:
        print(f"\nğŸ“‹ æœ€è¿‘çš„æ¸…ç†è®°å½•:")
        for i, log in enumerate(stats['recent_logs'][:5], 1):
            print(f"   {i}. {log['start_time'][:19]} - {log['cleanup_type']} - "
                  f"å‘ç° {log['orphans_found']} æ¸…ç† {log['orphans_cleaned']}")


async def update_config(storage_root: str, config_updates: dict):
    """æ›´æ–°æ¸…ç†é…ç½®"""
    print("âš™ï¸ æ›´æ–°æ¸…ç†é…ç½®...")
    
    manager = get_metadata_manager(storage_root)
    cleanup_manager = MetadataCleanupManager(manager)
    await cleanup_manager.initialize()
    
    await cleanup_manager.update_config(**config_updates)
    
    print("âœ… é…ç½®æ›´æ–°æˆåŠŸ")
    print("\nğŸ“‹ å½“å‰é…ç½®:")
    config = cleanup_manager.config
    print(f"   å¯ç”¨è‡ªåŠ¨æ¸…ç†: {config.enabled}")
    print(f"   å®½é™æœŸ: {config.grace_period}ç§’")
    print(f"   æ‰¹é‡å¤§å°: {config.batch_size}")
    print(f"   æ‰«æé—´éš”: {config.scan_interval}ç§’")
    print(f"   å•æ¬¡æœ€å¤§æ¸…ç†: {config.max_orphans_per_run}")
    print(f"   æ¸…ç†å‰å¤‡ä»½: {config.backup_before_cleanup}")
    print(f"   æ’é™¤æ¨¡å¼: {config.exclude_patterns}")


async def create_missing_metadata(storage_root: str, dry_run: bool = True):
    """ä¸ºç¼ºå¤±å…ƒæ•°æ®çš„æ–‡ä»¶åˆ›å»ºå…ƒæ•°æ®"""
    action = "è¯•è¿è¡Œåˆ›å»º" if dry_run else "åˆ›å»º"
    print(f"ğŸ“ å¼€å§‹{action}ç¼ºå¤±çš„å…ƒæ•°æ®...")
    
    manager = get_metadata_manager(storage_root)
    cleanup_manager = MetadataCleanupManager(manager)
    await cleanup_manager.initialize()
    
    # å…ˆæ£€æŸ¥ä¸€è‡´æ€§
    result = await cleanup_manager.check_consistency()
    missing_files = result['missing_metadata']
    
    if not missing_files:
        print("âœ… æ²¡æœ‰å‘ç°ç¼ºå¤±å…ƒæ•°æ®çš„æ–‡ä»¶")
        return
    
    print(f"ğŸ“Š å‘ç° {len(missing_files)} ä¸ªç¼ºå¤±å…ƒæ•°æ®çš„æ–‡ä»¶")
    
    if dry_run:
        print(f"\nğŸ“‹ å°†è¦åˆ›å»ºå…ƒæ•°æ®çš„æ–‡ä»¶:")
        for i, file_path in enumerate(missing_files[:10], 1):
            print(f"   {i:3d}. {file_path}")
        if len(missing_files) > 10:
            print(f"   ... è¿˜æœ‰ {len(missing_files) - 10} ä¸ª")
        print(f"\nâš ï¸  è¿™æ˜¯è¯•è¿è¡Œæ¨¡å¼ï¼Œè¦æ‰§è¡Œå®é™…åˆ›å»ºï¼Œè¯·æ·»åŠ  --execute å‚æ•°")
        return
    
    # æ‰§è¡Œåˆ›å»º
    created_count = 0
    errors = []
    
    for file_path in missing_files:
        try:
            full_path = cleanup_manager.storage_root / file_path
            if full_path.exists():
                file_size = full_path.stat().st_size
                await manager.create_metadata(file_path, file_size)
                created_count += 1
                print(f"   âœ… {file_path}")
            else:
                errors.append(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        except Exception as e:
            errors.append(f"åˆ›å»ºå…ƒæ•°æ®å¤±è´¥ {file_path}: {e}")
    
    print(f"\nğŸ“Š åˆ›å»ºç»“æœ:")
    print(f"   æˆåŠŸåˆ›å»º: {created_count}")
    print(f"   å¤±è´¥æ•°é‡: {len(errors)}")
    
    if errors:
        print(f"\nâŒ åˆ›å»ºé”™è¯¯:")
        for error in errors[:5]:
            print(f"   - {error}")


async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="å…ƒæ•°æ®æ¸…ç†å·¥å…·")
    parser.add_argument("--storage-root", default="./storage", help="å­˜å‚¨æ ¹ç›®å½•")
    
    subparsers = parser.add_subparsers(dest="command", help="å¯ç”¨å‘½ä»¤")
    
    # æ£€æŸ¥ä¸€è‡´æ€§
    check_parser = subparsers.add_parser("check", help="æ£€æŸ¥å…ƒæ•°æ®ä¸€è‡´æ€§")
    
    # æ¸…ç†å­¤å„¿å…ƒæ•°æ®
    cleanup_parser = subparsers.add_parser("cleanup", help="æ¸…ç†å­¤å„¿å…ƒæ•°æ®")
    cleanup_parser.add_argument("--execute", action="store_true", help="æ‰§è¡Œå®é™…æ¸…ç†ï¼ˆé»˜è®¤è¯•è¿è¡Œï¼‰")
    cleanup_parser.add_argument("--max-orphans", type=int, help="å•æ¬¡æœ€å¤§æ¸…ç†æ•°é‡")
    
    # æ˜¾ç¤ºç»Ÿè®¡
    stats_parser = subparsers.add_parser("stats", help="æ˜¾ç¤ºæ¸…ç†ç»Ÿè®¡")
    stats_parser.add_argument("--days", type=int, default=7, help="ç»Ÿè®¡å¤©æ•°")
    
    # æ›´æ–°é…ç½®
    config_parser = subparsers.add_parser("config", help="æ›´æ–°æ¸…ç†é…ç½®")
    config_parser.add_argument("--enabled", type=bool, help="å¯ç”¨/ç¦ç”¨è‡ªåŠ¨æ¸…ç†")
    config_parser.add_argument("--grace-period", type=int, help="å®½é™æœŸï¼ˆç§’ï¼‰")
    config_parser.add_argument("--batch-size", type=int, help="æ‰¹é‡å¤„ç†å¤§å°")
    config_parser.add_argument("--scan-interval", type=int, help="æ‰«æé—´éš”ï¼ˆç§’ï¼‰")
    config_parser.add_argument("--max-orphans-per-run", type=int, help="å•æ¬¡æœ€å¤§æ¸…ç†æ•°é‡")
    config_parser.add_argument("--backup-before-cleanup", type=bool, help="æ¸…ç†å‰æ˜¯å¦å¤‡ä»½")
    
    # åˆ›å»ºç¼ºå¤±å…ƒæ•°æ®
    create_parser = subparsers.add_parser("create-missing", help="ä¸ºç¼ºå¤±å…ƒæ•°æ®çš„æ–‡ä»¶åˆ›å»ºå…ƒæ•°æ®")
    create_parser.add_argument("--execute", action="store_true", help="æ‰§è¡Œå®é™…åˆ›å»ºï¼ˆé»˜è®¤è¯•è¿è¡Œï¼‰")
    
    args = parser.parse_args()
    
    # æ£€æŸ¥å­˜å‚¨ç›®å½•
    storage_root = Path(args.storage_root)
    if not storage_root.exists():
        print(f"âŒ å­˜å‚¨ç›®å½•ä¸å­˜åœ¨: {storage_root}")
        return
    
    try:
        if args.command == "check":
            await check_consistency(str(storage_root))
        
        elif args.command == "cleanup":
            await cleanup_orphans(
                str(storage_root), 
                dry_run=not args.execute,
                max_orphans=args.max_orphans
            )
        
        elif args.command == "stats":
            await show_stats(str(storage_root), args.days)
        
        elif args.command == "config":
            config_updates = {}
            if args.enabled is not None:
                config_updates["enabled"] = args.enabled
            if args.grace_period is not None:
                config_updates["grace_period"] = args.grace_period
            if args.batch_size is not None:
                config_updates["batch_size"] = args.batch_size
            if args.scan_interval is not None:
                config_updates["scan_interval"] = args.scan_interval
            if args.max_orphans_per_run is not None:
                config_updates["max_orphans_per_run"] = args.max_orphans_per_run
            if args.backup_before_cleanup is not None:
                config_updates["backup_before_cleanup"] = args.backup_before_cleanup
            
            if config_updates:
                await update_config(str(storage_root), config_updates)
            else:
                print("âŒ æ²¡æœ‰æŒ‡å®šè¦æ›´æ–°çš„é…ç½®é¡¹")
        
        elif args.command == "create-missing":
            await create_missing_metadata(str(storage_root), dry_run=not args.execute)
        
        else:
            print("âŒ è¯·æŒ‡å®šä¸€ä¸ªå‘½ä»¤ã€‚ä½¿ç”¨ -h æŸ¥çœ‹å¸®åŠ©")
            parser.print_help()
    
    except Exception as e:
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())